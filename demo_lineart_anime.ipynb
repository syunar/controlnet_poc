{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO9eMmaMJ/vwvO+Q93Mw/M8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FdOx_D8pzR86","executionInfo":{"status":"ok","timestamp":1686578205487,"user_tz":-420,"elapsed":243262,"user":{"displayName":"Supakorn Dumnin","userId":"15402223538356083550"}},"outputId":"2ed9a6f0-ecf0-4b3b-b550-1983578320ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into 'controlnet_poc'...\n","remote: Enumerating objects: 2416, done.\u001b[K\n","remote: Total 2416 (delta 0), reused 0 (delta 0), pack-reused 2416\n","Receiving objects: 100% (2416/2416), 65.19 MiB | 15.38 MiB/s, done.\n","Resolving deltas: 100% (997/997), done.\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m708.1/708.1 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m132.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.9/200.9 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for basicsr (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n","torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n","torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mThe following additional packages will be installed:\n","  libaria2-0 libc-ares2\n","The following NEW packages will be installed:\n","  aria2 libaria2-0 libc-ares2\n","0 upgraded, 3 newly installed, 0 to remove and 38 not upgraded.\n","Need to get 1,475 kB of archives.\n","After this operation, 5,959 kB of additional disk space will be used.\n","Selecting previously unselected package libc-ares2:amd64.\n","(Reading database ... 122541 files and directories currently installed.)\n","Preparing to unpack .../libc-ares2_1.15.0-1ubuntu0.2_amd64.deb ...\n","Unpacking libc-ares2:amd64 (1.15.0-1ubuntu0.2) ...\n","Selecting previously unselected package libaria2-0:amd64.\n","Preparing to unpack .../libaria2-0_1.35.0-1build1_amd64.deb ...\n","Unpacking libaria2-0:amd64 (1.35.0-1build1) ...\n","Selecting previously unselected package aria2.\n","Preparing to unpack .../aria2_1.35.0-1build1_amd64.deb ...\n","Unpacking aria2 (1.35.0-1build1) ...\n","Setting up libc-ares2:amd64 (1.15.0-1ubuntu0.2) ...\n","Setting up libaria2-0:amd64 (1.35.0-1build1) ...\n","Setting up aria2 (1.35.0-1build1) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n","\u001b[0m\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","e7f1df|\u001b[1;32mOK\u001b[0m  |   201MiB/s|/content/controlnet_poc/models/anything-v3-full.safetensors\n","\n","Status Legend:\n","(OK):download completed.\n","\u001b[0m\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","a2dcd5|\u001b[1;32mOK\u001b[0m  |   237MiB/s|/content/controlnet_poc/models/control_v11p_sd15s2_lineart_anime.pth\n","\n","Status Legend:\n","(OK):download completed.\n","/content/controlnet_poc\n"]}],"source":["%cd /content\n","!git clone https://github.com/syunar/controlnet_poc.git\n","!pip install -q gradio einops transformers open_clip_torch pytorch_lightning==1.7.7 omegaconf xformers==0.0.18 triton==2.0.0 basicsr safetensors fvcore\n","!apt -y install -qq aria2\n","\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/anything-v3-full.safetensors -d /content/controlnet_poc/models -o anything-v3-full.safetensors\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15s2_lineart_anime.pth -d /content/controlnet_poc/models -o control_v11p_sd15s2_lineart_anime.pth \n","\n","%cd /content/controlnet_poc"]},{"cell_type":"code","source":["from share import *\n","import config\n","from cldm.hack import hack_everything\n","\n","\n","hack_everything(clip_skip=2)\n","\n","\n","import cv2\n","import einops\n","import gradio as gr\n","import numpy as np\n","import torch\n","import random\n","\n","from pytorch_lightning import seed_everything\n","from annotator.util import resize_image, HWC3\n","from annotator.lineart_anime import LineartAnimeDetector\n","from cldm.model import create_model, load_state_dict\n","from cldm.ddim_hacked import DDIMSampler\n","\n","preprocessor = None\n","\n","\n","model_name = 'control_v11p_sd15s2_lineart_anime'\n","model = create_model(f'./models/{model_name}.yaml').cpu()\n","model.load_state_dict(load_state_dict('./models/anything-v3-full.safetensors', location='cuda'), strict=False)\n","model.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\n","model = model.cuda()\n","ddim_sampler = DDIMSampler(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QTuKyRgy0npm","outputId":"888a9dd4-1cb1-4e3f-8f09-13e4e7444066"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["logging improved.\n","Enabled clip hacks.\n","ControlLDM: Running in eps-prediction mode\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n","DiffusionWrapper has 859.52 M params.\n","making attention of type 'vanilla-xformers' with 512 in_channels\n","building MemoryEfficientAttnBlock with 512 in_channels...\n","Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n","making attention of type 'vanilla-xformers' with 512 in_channels\n","building MemoryEfficientAttnBlock with 512 in_channels...\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n","Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n","Loaded model config from [./models/control_v11p_sd15s2_lineart_anime.yaml]\n","Loaded state_dict from [./models/anything-v3-full.safetensors]\n","Loaded state_dict from [./models/control_v11p_sd15s2_lineart_anime.pth]\n"]}]},{"cell_type":"code","source":["def process(det, image_path, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, strength, scale, seed, eta):\n","    global preprocessor\n","    \n","    input_image = cv2.imread(image_path)   # reads an image in the BGR format\n","    input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)   # BGR -> RGB\n","\n","    if det == 'Lineart_Anime':\n","        if not isinstance(preprocessor, LineartAnimeDetector):\n","            preprocessor = LineartAnimeDetector()\n","\n","    with torch.no_grad():\n","        input_image = HWC3(input_image)\n","\n","        if det == 'None':\n","            detected_map = input_image.copy()\n","        else:\n","            detected_map = preprocessor(resize_image(input_image, detect_resolution))\n","            detected_map = HWC3(detected_map)\n","\n","        img = resize_image(input_image, image_resolution)\n","        H, W, C = img.shape\n","\n","        detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n","\n","        control = 1.0 - torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n","        control = torch.stack([control for _ in range(num_samples)], dim=0)\n","        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n","\n","        if seed == -1:\n","            seed = random.randint(0, 65535)\n","        seed_everything(seed)\n","\n","        if config.save_memory:\n","            model.low_vram_shift(is_diffusing=False)\n","\n","        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n","        un_cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n","        shape = (4, H // 8, W // 8)\n","\n","        if config.save_memory:\n","            model.low_vram_shift(is_diffusing=True)\n","\n","        model.control_scales = [strength] * 13\n","        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n","                                                     shape, cond, verbose=False, eta=eta,\n","                                                     unconditional_guidance_scale=scale,\n","                                                     unconditional_conditioning=un_cond)\n","\n","        if config.save_memory:\n","            model.low_vram_shift(is_diffusing=False)\n","\n","        x_samples = model.decode_first_stage(samples)\n","        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n","\n","        results = [x_samples[i] for i in range(num_samples)]\n","    return [detected_map] + results"],"metadata":{"id":"jBrEjZeoBQjA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["res = process(det='None', \n","        image_path='/content/01.png', \n","        prompt='', \n","        a_prompt='masterpiece, best quality, ultra-detailed, illustration, disheveled hair', \n","        n_prompt='longbody, lowres, bad anatomy, bad hands, missing fingers, pubic hair,extra digit, fewer digits, cropped, worst quality, low quality', \n","        num_samples=1, \n","        image_resolution=512, \n","        detect_resolution=512, \n","        ddim_steps=20, \n","        strength=1.0, \n","        scale=9.0, \n","        seed=1234, \n","        eta=1.0)\n","\n","import torch\n","import gc\n","gc.collect()\n","with torch.no_grad():\n","    torch.cuda.empty_cache()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"669PsNXY37O3","executionInfo":{"status":"error","timestamp":1686580254021,"user_tz":-420,"elapsed":41,"user":{"displayName":"Supakorn Dumnin","userId":"15402223538356083550"}},"outputId":"7b42f483-71f0-4527-93bc-258e5ea07021"},"execution_count":32,"outputs":[{"output_type":"error","ename":"UnboundLocalError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-8fe52e14ae44>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m res = process(det='None', \n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mimage_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/01.png'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0ma_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'masterpiece, best quality, ultra-detailed, illustration, disheveled hair'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mn_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'longbody, lowres, bad anatomy, bad hands, missing fingers, pubic hair,extra digit, fewer digits, cropped, worst quality, low quality'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-31-3c739dd26b00>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(det, image_path, prompt, a_prompt, n_prompt, num_samples, image_resolution, detect_resolution, ddim_steps, strength, scale, seed, eta)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0minput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# reads an image in the BGR format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0minput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# BGR -> RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdet\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Lineart_Anime'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'img' referenced before assignment"]}]},{"cell_type":"code","source":["from matplotlib import pyplot as plt\n","# Create the figure and subplots\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n","\n","# Display image 1 on the left subplot\n","ax1.imshow(res[0])\n","ax1.set_title('Image 1')\n","\n","# Display image 2 on the right subplot\n","ax2.imshow(res[1])\n","ax2.set_title('Image 2')\n","\n","# Adjust the spacing between subplots\n","plt.subplots_adjust(wspace=0.3)\n","\n","# Show the figure\n","plt.show()"],"metadata":{"id":"5Sl45LQc7Pqw","executionInfo":{"status":"aborted","timestamp":1686580254023,"user_tz":-420,"elapsed":14,"user":{"displayName":"Supakorn Dumnin","userId":"15402223538356083550"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"M1XuC3Ig7YvI"},"execution_count":null,"outputs":[]}]}